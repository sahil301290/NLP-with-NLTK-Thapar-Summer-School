{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NitinAShelke/NLP-with-NLTK-Thapar-Summer-School/blob/main/Day_3_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_aJe0eWZob9"
      },
      "source": [
        "# Feature Extraction and Word Embeddings\n",
        "\n",
        "#### by Nitin Shelke"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1M_0ASQZocB"
      },
      "source": [
        "# Bag-of-words vectorization "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4veENu3ZocC"
      },
      "source": [
        "Whenever we apply any algorithm to textual data, we need to convert the text to a numeric form. Hence, there arises a need for some pre-processing techniques that can convert our text to numbers. Both bag-of-words (BOW) and TFIDF are pre-processing techniques that can generate a numeric form from an input text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AquEXxUyZocD"
      },
      "source": [
        "### Bag-of-Words:\n",
        "\n",
        "The bag-of-words model converts text into fixed-length vectors by counting how many times each word appears.\n",
        "\n",
        "Whenever we apply any algorithm to textual data, we need to convert the text to a numeric form. Hence, there arises a need for some pre-processing techniques that can convert our text to numbers. Both bag-of-words (BOW) and TFIDF are pre-processing techniques that can generate a numeric form from an input text.\n",
        "\n",
        "\n",
        "\n",
        "Let us illustrate this with an example. Consider that we have the following sentences:\n",
        "\n",
        "DOC1: Text processing is necessary.\n",
        "\n",
        "DOC2: Text processing is necessary and important.\n",
        "\n",
        "DOC3: Text processing is easy.\n",
        "\n",
        "If we take out the unique words in all these sentences, the vocabulary will consist of these 7 words: {‘Text’, ’processing’, ’is’, ’necessary’, ’and’, ’important, ’easy’}.\n",
        "\n",
        "To carry out bag-of-words, we will simply have to count the number of times each word appears in each of the documents.\n",
        "\n",
        "\n",
        "![image.png](attachment:image.png)\n",
        "\n",
        "Hence, we have the following vectors for each of the documents of fixed length -7:\n",
        "\n",
        "Document 1: [1,1,1,1,0,0,0]\n",
        "\n",
        "Document 2: [1,1,1,1,1,1,0]\n",
        "\n",
        "Document 3: [1,1,1,0,0,0,1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6eo1gqK1ZocD"
      },
      "source": [
        "##### Limitations of Bag-of-Words:\n",
        "\n",
        "If we deploy bag-of-words to generate vectors for large documents, the vectors would be of large sizes and would also have too many null values leading to the creation of sparse vectors.\n",
        "\n",
        "Bag-of-words does not bring in any information on the meaning of the text. For example, if we consider these two sentences – “Text processing is easy but tedious.” and “Text processing is tedious but easy.” – a bag-of-words model would create the same vectors for both of them, even though they have different meanings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ekUW547ZocE"
      },
      "outputs": [],
      "source": [
        "# Example 1\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "corpus = ['Text processing is necessary.', 'Text processing is necessary and important.', 'Text processing is easy.']\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "print(vectorizer.get_feature_names())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YLZn9gpsZocF"
      },
      "outputs": [],
      "source": [
        "print(X.toarray())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QX0xEDs-ZocG"
      },
      "outputs": [],
      "source": [
        "# Example 2\n",
        "text = [\"i love NLP\",\n",
        "        \"NLP is future\",\n",
        "        \"i will learn in 2 months\"]\n",
        "vectorizer = CountVectorizer()\n",
        "count_matrix = vectorizer.fit_transform(text)\n",
        "count_array = count_matrix.toarray()\n",
        "df = pd.DataFrame(data=count_array,columns = vectorizer.get_feature_names())\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g1ZcOiUBZocG"
      },
      "source": [
        "# TFIDF Vectorization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTI_VJ9fZocH"
      },
      "source": [
        "TFIDF works by proportionally increasing the number of times a word appears in the document but is counterbalanced by the number of documents in which it is present. Hence, words like ‘this’, ’are’ etc., that are commonly present in all the documents are not given a very high rank. However, a word that is present too many times in a few of the documents will be given a higher rank as it might be indicative of the context of the document.\n",
        "\n",
        "###### Term Frequency:\n",
        "\n",
        "Term frequency is defined as the number of times a word (i) appears in a document (j) divided by the total number of words in the document.\n",
        "\n",
        "![image.png](attachment:image.png)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KNN54MSLZocH"
      },
      "source": [
        "###### Inverse Document Frequency:\n",
        "\n",
        "Inverse document frequency refers to the log of the total number of documents divided by the number of documents that contain the word. The logarithm is added to dampen the importance of a very high value of IDF.\n",
        "\n",
        "\n",
        "![image.png](attachment:image.png)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3n1TJg7iZocI"
      },
      "source": [
        "###### TFIDF\n",
        "\n",
        "It is computed by multiplying the term frequency with the inverse document frequency.\n",
        "\n",
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxhgzh6oZocI"
      },
      "source": [
        "Let us now see an illustration of TFIDF in the following sentences, that we refer to as documents.\n",
        "\n",
        "Document 1: Text processing is necessary.\n",
        "\n",
        "Document 2: Text processing is necessary and important.\n",
        "\n",
        "![image.png](attachment:image.png)\n",
        "\n",
        "The above table shows how the TFIDF of some words are zero and some words are non-zero depending on their frequency in the document and across all documents.\n",
        "\n",
        "The limitation of TFIDF is again that this vectorization doesn’t help in bringing in the contextual meaning of the words as it is just based on the frequency."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0sUfeUIWZocI"
      },
      "outputs": [],
      "source": [
        "# TFIDF Vectorization code\n",
        "\n",
        "#Example 1\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "print(X.toarray())\n",
        "\n",
        "# The above array represents the vectors created for our 3 documents using the TFIDF vectorization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "On5rh_9QZocI"
      },
      "outputs": [],
      "source": [
        "# Example 2\n",
        "import pandas as pd\n",
        "text = [\"i love the NLP\",\n",
        "        \"NLP is the future\",\n",
        "        \"i will learn the NLP\"]\n",
        "vectorizer = TfidfVectorizer()\n",
        "matrix = vectorizer.fit_transform(text)\n",
        "count_array = matrix.toarray()\n",
        "df = pd.DataFrame(data=count_array,columns = vectorizer.get_feature_names())\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SzuoAadBZocJ"
      },
      "source": [
        "“NLP”, ”the” came in all the three documents hence it has a smaller vector value. ”love” has a higher vector value since it appeared only once in a document."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8OvIo9tZocJ"
      },
      "source": [
        "# Word Embedding "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JEZUkhdIZocJ"
      },
      "source": [
        "Imagine I have 2 words “love” and “like”, these two words have almost similar meanings but according to TF-IDF and BOW model these two will have separate feature values and these 2 words will be treated completely different.\n",
        "\n",
        "TF-IDF, BOW model completely depends on the frequency of occurrence, it doesn’t take the meaning of words into consideration, hence above-discussed techniques are failed to capture the context and meaning of sentences.\n",
        "\n",
        "“I like you” and “I love you” will have completely different feature vectors according to TF-IDF and BOW model, but that’s not correct.\n",
        "\n",
        "note: while working with some classification task we would have big raw data and if we keep considering every synonym as a different word it will generate humongous numbers of tokens and this will cause numbers of features to get out of control.\n",
        "\n",
        "Word embedding is a feature learning technique where words are mapped to vectors using their contextual hierarchy. similar words will have identical feature vectors.\n",
        "\n",
        "![image.png](attachment:image.png)\n",
        "\n",
        "\n",
        "As you notice, cats and kitten are placed very closely since they are related.\n",
        "\n",
        "word embedding is trained on more than 6 billion words using shallow neural networks.\n",
        "\n",
        "word2vec has 2 algorithms\n",
        "\n",
        "1/CBOW\n",
        "\n",
        "2/Skip-Gram\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hFHTmrqzZocJ"
      },
      "source": [
        "# Implementing Glove word embedding using python:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8NWHTfDXZocK"
      },
      "source": [
        "don’t worry we don’t need to train word2vec, we will use pre-trained word vectors.\n",
        "\n",
        "A pre-trained word vector is a text file containing billions of words with their vectors. we only need to map words from our data with the words in the word vector in order to get the vectors.\n",
        "\n",
        "Pre-trained word vector file come in (50,100,200,300) dimension. here dimension is the length of the vector of each word in vector space. more dimension means more information about that word but bigger dimension takes longer time for model training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4zN5htp0ZocK"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Loading glove word embedding of 100 dimensions into a dictionary:\n",
        "\n",
        "import numpy as np\n",
        "glove_vectors = dict()\n",
        "file = open('glove.6B.100d.txt', encoding = 'utf-8')\n",
        "for line in file:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    vectors = np.asarray(values[1:])\n",
        "    glove_vectors[word] = vectors\n",
        "file.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cEUN5T2NZocK"
      },
      "outputs": [],
      "source": [
        "#we can use the get() method to glove_vectors to get the vectors\n",
        "glove_vectors.get('house')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GJs5H5qqZocK"
      },
      "outputs": [],
      "source": [
        "# Creating a function that takes every sentence and returns word vectors:\n",
        "vec_dimension = 100\n",
        "def get_vec(x):\n",
        "    arr  = np.zeros(vec_dimension)\n",
        "    text = str(x).split()\n",
        "    for t in text:\n",
        "        try:\n",
        "            vec = glove_vectors.get(t).astype(float)\n",
        "            arr = arr + vec\n",
        "        except:\n",
        "            pass\n",
        "    arr = arr.reshape(1,-1)[0]\n",
        "    return(arr/len(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f4mtVyPhZocL"
      },
      "outputs": [],
      "source": [
        "x = ['I love you',\n",
        "     'I love NLP and i will try to learn',\n",
        "    'this is word embedding']\n",
        "features = get_vec(x)\n",
        "features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IhWbMFTaZocL"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "Day 3_1.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}