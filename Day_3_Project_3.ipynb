{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NitinAShelke/NLP-with-NLTK-Thapar-Summer-School/blob/main/Day_3_Project_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0nRrZDgDl3bE"
      },
      "source": [
        "# Text Summarization \n",
        "#### By Nitin Arvind Shelke"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jON96oyjl3bI"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "import re\n",
        "from nltk.corpus import stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pMl0xN32l3bJ"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"tennis.csv\")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F7fgA0oul3bK"
      },
      "outputs": [],
      "source": [
        "df['article_text'][1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FJ8mdi2Nl3bK"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "sentences = []\n",
        "for s in df['article_text']:\n",
        "    sentences.append(sent_tokenize(s))\n",
        "\n",
        "sentences = [y for x in sentences for y in x]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A7j07b23l3bL"
      },
      "outputs": [],
      "source": [
        "# use the Glove method for word representation\n",
        "word_embeddings = {}\n",
        "f = open('glove.6B.100d.txt', encoding='utf-8')\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    word_embeddings[word] = coefs\n",
        "f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vFa60Esll3bL"
      },
      "outputs": [],
      "source": [
        "clean_sentences = pd.Series(sentences).str.replace(\"[^a-zA-Z]\", \" \")\n",
        "clean_sentences = [s.lower() for s in clean_sentences]\n",
        "stop_words = stopwords.words('english')\n",
        "def remove_stopwords(sen):\n",
        "    sen_new = \" \".join([i for i in sen if i not in stop_words])\n",
        "    return sen_new\n",
        "clean_sentences = [remove_stopwords(r.split()) for r in clean_sentences]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KFU-QxZsl3bM"
      },
      "outputs": [],
      "source": [
        "# create vectors for the sentences:\n",
        "sentence_vectors = []\n",
        "for i in clean_sentences:\n",
        "    if len(i) != 0:\n",
        "        v = sum([word_embeddings.get(w, np.zeros((100,))) for w in i.split()])/(len(i.split())+0.001)\n",
        "    else:\n",
        "        v = np.zeros((100,))\n",
        "    sentence_vectors.append(v)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OOOwtYmrl3bN"
      },
      "outputs": [],
      "source": [
        "# Finding Similarities to Summarize Text\n",
        "sim_mat = np.zeros([len(sentences), len(sentences)])\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "for i in range(len(sentences)):\n",
        "    for j in range(len(sentences)):\n",
        "        if i != j:\n",
        "            sim_mat[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,100), sentence_vectors[j].reshape(1,100))[0,0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lXe4eZUUl3bN"
      },
      "outputs": [],
      "source": [
        "# convert the sim_mat similarity matrix into the graph, the nodes in this graph will represent the sentences and \n",
        "#the edges will represent the similarity scores between the sentences:\n",
        "import networkx as nx\n",
        "\n",
        "nx_graph = nx.from_numpy_array(sim_mat)\n",
        "scores = nx.pagerank(nx_graph)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XCYULlKGl3bO"
      },
      "outputs": [],
      "source": [
        "#Now, letâ€™s summarize text:\n",
        "ranked_sentences = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)\n",
        "for i in range(5):\n",
        "    print(\"ARTICLE:\")\n",
        "    print(df['article_text'][i])\n",
        "    print('\\n')\n",
        "    print(\"SUMMARY:\")\n",
        "    print(ranked_sentences[i][1])\n",
        "    print('\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4je3FrNBl3bO"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "Day_3_Project_3.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}