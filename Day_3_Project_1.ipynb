{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NitinAShelke/NLP-with-NLTK-Thapar-Summer-School/blob/main/Day_3_Project_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIquuesignoa"
      },
      "source": [
        "# Movie Reviews Sentiment Analysis \n",
        "\n",
        "\n",
        "#### by Nitin Arvind Shelke"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Pg6FtdCgnoe"
      },
      "outputs": [],
      "source": [
        "import numpy as np \n",
        "import pandas as pd \n",
        "import re \n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import SnowballStemmer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB,MultinomialNB,BernoulliNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# dataset link\n",
        "https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews"
      ],
      "metadata": {
        "id": "cxdHRdx-hXea"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bqtt1hJmgnog"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv('IMDB-Dataset.csv')\n",
        "print(data.shape)\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ByJ1zRlgnog"
      },
      "outputs": [],
      "source": [
        "data.sentiment.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I9WtfhA3gnog"
      },
      "outputs": [],
      "source": [
        "data.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4o1uIxaognoh"
      },
      "outputs": [],
      "source": [
        "data.sentiment.replace('positive',1,inplace=True)\n",
        "data.sentiment.replace('negative',0,inplace=True)\n",
        "data.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iU_TD6ojgnoh"
      },
      "source": [
        "###### STEPS TO CLEAN THE REVIEWS :\n",
        "Remove HTML tags\n",
        "\n",
        "Remove special characters\n",
        "\n",
        "Convert everything to lowercase\n",
        "\n",
        "Remove stopwords\n",
        "\n",
        "Stemming"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "shLoOrAhgnoi"
      },
      "outputs": [],
      "source": [
        "# Remove HTML tags\n",
        "def clean(text):\n",
        "    cleaned = re.compile(r'<.*?>')\n",
        "    return re.sub(cleaned,'',text)\n",
        "\n",
        "data.review = data.review.apply(clean)\n",
        "data.review[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "joGmuk3Ggnoj"
      },
      "outputs": [],
      "source": [
        "# Remove special characters\n",
        "def is_special(text):\n",
        "    rem = ''\n",
        "    for i in text:\n",
        "        if i.isalnum():\n",
        "            rem = rem + i\n",
        "        else:\n",
        "            rem = rem + ' '\n",
        "    return rem\n",
        "\n",
        "data.review = data.review.apply(is_special)\n",
        "data.review[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y0dk6kA3gnoj"
      },
      "outputs": [],
      "source": [
        "# lower case conversion\n",
        "def to_lower(text):\n",
        "    return text.lower()\n",
        "\n",
        "data.review = data.review.apply(to_lower)\n",
        "data.review[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KnEgd1ocgnok"
      },
      "outputs": [],
      "source": [
        "# Remove stopwords\n",
        "def rem_stopwords(text):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = word_tokenize(text)\n",
        "    return [w for w in words if w not in stop_words]\n",
        "\n",
        "data.review = data.review.apply(rem_stopwords)\n",
        "data.review[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zRnvZCsTgnok"
      },
      "outputs": [],
      "source": [
        "# Stem the words\n",
        "\n",
        "def stem_txt(text):\n",
        "    ss = SnowballStemmer('english')\n",
        "    return \" \".join([ss.stem(w) for w in text])\n",
        "\n",
        "data.review = data.review.apply(stem_txt)\n",
        "data.review[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZYhBCZjDgnol"
      },
      "outputs": [],
      "source": [
        "# preprocessed data\n",
        "\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JlUIaQAegnol"
      },
      "outputs": [],
      "source": [
        "# Bag Of Words (BOW) Feature extraction\n",
        "X = np.array(data.iloc[:,0].values)\n",
        "y = np.array(data.sentiment.values)\n",
        "cv = CountVectorizer(max_features = 1000)\n",
        "X = cv.fit_transform(data.review).toarray()\n",
        "print(\"X.shape = \",X.shape)\n",
        "print(\"y.shape = \",y.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kr4kPsGRgnol"
      },
      "outputs": [],
      "source": [
        "# Split the dataset\n",
        "\n",
        "trainx,testx,trainy,testy = train_test_split(X,y,test_size=0.2,random_state=9)\n",
        "print(\"Train shapes : X = {}, y = {}\".format(trainx.shape,trainy.shape))\n",
        "print(\"Test shapes : X = {}, y = {}\".format(testx.shape,testy.shape))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ZaN5EJwgnol"
      },
      "outputs": [],
      "source": [
        "# Defining the models and Training them\n",
        "\n",
        "gnb,mnb,bnb = GaussianNB(),MultinomialNB(alpha=1.0,fit_prior=True),BernoulliNB(alpha=1.0,fit_prior=True)\n",
        "gnb.fit(trainx,trainy)\n",
        "mnb.fit(trainx,trainy)\n",
        "bnb.fit(trainx,trainy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kl-EDunognom"
      },
      "outputs": [],
      "source": [
        "# Prediction and accuracy metrics to choose best model\n",
        "ypg = gnb.predict(testx)\n",
        "ypm = mnb.predict(testx)\n",
        "ypb = bnb.predict(testx)\n",
        "\n",
        "print(\"Gaussian = \",accuracy_score(testy,ypg))\n",
        "print(\"Multinomial = \",accuracy_score(testy,ypm))\n",
        "print(\"Bernoulli = \",accuracy_score(testy,ypb))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qd3qjJIognom"
      },
      "outputs": [],
      "source": [
        "pickle.dump(bnb,open('model1.pkl','wb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VuKiw0HQgnom"
      },
      "outputs": [],
      "source": [
        "rev =  \"\"\"Terrible. Complete trash. Brainless tripe. Insulting to anyone who isn't an 8 year old fan boy. Im actually pretty disgusted that this movie is making the money it is - what does it say about the people who brainlessly hand over the hard earned cash to be 'entertained' in this fashion and then come here to leave a positive 8.8 review?? Oh yes, they are morons. Its the only sensible conclusion to draw. How anyone can rate this movie amongst the pantheon of great titles is beyond me.\n",
        "\n",
        "So trying to find something constructive to say about this title is hard...I enjoyed Iron Man? Tony Stark is an inspirational character in his own movies but here he is a pale shadow of that...About the only 'hook' this movie had into me was wondering when and if Iron Man would knock Captain America out...Oh how I wished he had :( What were these other characters anyways? Useless, bickering idiots who really couldn't organise happy times in a brewery. The film was a chaotic mish mash of action elements and failed 'set pieces'...\n",
        "\n",
        "I found the villain to be quite amusing.\n",
        "\n",
        "And now I give up. This movie is not robbing any more of my time but I felt I ought to contribute to restoring the obvious fake rating and reviews this movie has been getting on IMDb.\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wWeYXfSVgnon"
      },
      "outputs": [],
      "source": [
        "rev"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dgundiXLgnon"
      },
      "outputs": [],
      "source": [
        "f1 = clean(rev)\n",
        "f2 = is_special(f1)\n",
        "f3 = to_lower(f2)\n",
        "f4 = rem_stopwords(f3)\n",
        "f5 = stem_txt(f4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m4c16lVIgnon"
      },
      "outputs": [],
      "source": [
        "bow,words = [],word_tokenize(f5)\n",
        "for word in words:\n",
        "    bow.append(words.count(word))\n",
        "#np.array(bow).reshape(1,3000)\n",
        "#bow.shape\n",
        "word_dict = cv.vocabulary_\n",
        "pickle.dump(word_dict,open('bow.pkl','wb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4WHlyOB-gnon"
      },
      "outputs": [],
      "source": [
        "inp = []\n",
        "for i in word_dict:\n",
        "    inp.append(f5.count(i[0]))\n",
        "y_pred = bnb.predict(np.array(inp).reshape(1,1000))\n",
        "y_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8fqh7_gagnoo"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "16tknPNqgnoo"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "Day 3 Project_1.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}