{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NitinAShelke/NLP-with-NLTK-Thapar-Summer-School/blob/main/NLP_Day_1_Hands_on.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Gfjn6HvX0ro"
      },
      "outputs": [],
      "source": [
        "# Import nltk\n",
        "\n",
        "import nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aZjrXOSzX0ry"
      },
      "outputs": [],
      "source": [
        "# dowload all the packages\n",
        "nltk.download()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ETTqnvFX0rz"
      },
      "source": [
        "# String Manipulation "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AClskSIlX0r2"
      },
      "outputs": [],
      "source": [
        "# Slicing\n",
        "#lower()\n",
        "#upper()\n",
        "#title()\n",
        "#capitalize()\n",
        "# split()\n",
        "#join()\n",
        "#count()\n",
        "#find()\n",
        "#strip()\n",
        "#replace()\n",
        "#endswith()\n",
        "#startswith()\n",
        "#islower()\n",
        "#isupper()\n",
        "#isalpha()\n",
        "#isalnum() \n",
        "#isdigit() \n",
        "#istitle() \n",
        "#in or not in\n",
        "#len"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A-v7cd1DX0r3"
      },
      "outputs": [],
      "source": [
        "#Loading Book\n",
        "from nltk.book import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4iFuR9DiX0r4"
      },
      "outputs": [],
      "source": [
        "#Counting vocab\n",
        "len(text3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gY0WhOOrX0r6"
      },
      "outputs": [],
      "source": [
        "#set method\n",
        "len(set(text3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ycJpZgUyX0r8"
      },
      "outputs": [],
      "source": [
        "\n",
        "txt=\"The food is delicious. The food is not tasty\"\n",
        "txt=nltk.Text( txt.split())\n",
        "\n",
        "print(set(txt))\n",
        "print(len(set(txt)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R-6h4rNUX0sA"
      },
      "outputs": [],
      "source": [
        "#Making Decisions and Taking Control with single condition\n",
        "\n",
        "[w for w in set(text1) if w.endswith('ab')]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vo7HXMwwX0sC"
      },
      "outputs": [],
      "source": [
        "#[term for term in set(text) if 'me' in term]\n",
        "#[item for item in set(text) if item.istitle()]\n",
        "#[item for item in set(sent) if item.isdigit()]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y68CBSihX0sD"
      },
      "outputs": [],
      "source": [
        "# Making Decisions and Taking Control with Two Conditions\n",
        "[wd for wd in set(text1) if wd.istitle() and len(wd) > 13]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6_6tzdlrX0sE"
      },
      "outputs": [],
      "source": [
        "#[w for w in set(text) if not w.islower()]\n",
        "#[t for t in set(text) if 'th' in t or 'o' in t]\n",
        "#[t for t in set(text) if 'th' in t and 'o' in t]\n",
        "#[len(w) for w in text]\n",
        "#[w.upper() for w in text]\n",
        "#len(set([word.lower() for word in text]))\n",
        "#len(set([word.lower() for word in text if word.isalpha()]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7EedWoiaX0sF"
      },
      "outputs": [],
      "source": [
        "# Fine Grained Selection of Words\n",
        "#Write a code to find the words from the vocabulary of the text 2 that are more than 14 characters long.\n",
        "vocb = set(text2) \n",
        "words_formed = [x for x in vocb if len(x) > 14 ] \n",
        "sorted(words_formed )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-BTxN0Z7X0sG"
      },
      "source": [
        "# Basic NLTK methods "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FINtb7-oX0sG"
      },
      "source": [
        "###### concordance\n",
        "A concordance view shows us every occurrence of a given word, together with some context. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BbEMwMfPX0sH"
      },
      "outputs": [],
      "source": [
        "text1.concordance(\"monstrous\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-aArMREUX0sI"
      },
      "source": [
        "###### similar\n",
        "A similar method permits us to see what words appear in a similar range of contexts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kp3Hlvn8X0sI"
      },
      "outputs": [],
      "source": [
        "text1.similar(\"monstrous\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58jNdIPSX0sJ"
      },
      "source": [
        "###### common_contexts()\n",
        "\n",
        "The term common_contexts allows us to examine just the contexts that are shared by two or more words, such as monstrous and very. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oaTvkLgVX0sK"
      },
      "outputs": [],
      "source": [
        "text2.common_contexts([\"monstrous\", \"very\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6SBzik2KX0sK"
      },
      "source": [
        "###### Lexical Diversity \n",
        "let's calculate a measure of the lexical richness of the text. The next example shows us that the number of distinct words is just 6% of the total number of words, or equivalently that each word is used 16 times on average"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kloh2hP1X0sL"
      },
      "outputs": [],
      "source": [
        "len(set(text3)) / len(text3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "09-IFx7xX0sM"
      },
      "outputs": [],
      "source": [
        "def lexical_diversity(text):\n",
        "    return len(set(text)) / len(text) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i2czh4KnX0sN"
      },
      "outputs": [],
      "source": [
        "lexical_diversity(text3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DU-x5scpX0sN"
      },
      "source": [
        "Next, let's focus on particular words. We can count how often a word occurs in a text, and compute what percentage of the text is taken up by a specific word:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d08emRUyX0sO"
      },
      "outputs": [],
      "source": [
        "print(text3.count(\"smote\"))\n",
        "print(100 * text4.count('a') / len(text4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6nfF3UDhX0sP"
      },
      "outputs": [],
      "source": [
        "def percentage(count, total):\n",
        "    return 100 * count / total"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ue9OkTZaX0sP"
      },
      "outputs": [],
      "source": [
        "percentage(text4.count('a'), len(text4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0A2ydanX0sQ"
      },
      "source": [
        "### Frequency Distributions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eZJPA4QXX0sQ"
      },
      "outputs": [],
      "source": [
        "fdist1 = FreqDist(text1)\n",
        "print(fdist1) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p1EHe32UX0sR"
      },
      "outputs": [],
      "source": [
        "fdist1.most_common(50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H5tzZnzBX0sS"
      },
      "outputs": [],
      "source": [
        "fdist1['whale']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VtLIa4mjX0sS"
      },
      "outputs": [],
      "source": [
        "fdist1['monstrous']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wcdrn6clX0sT"
      },
      "outputs": [],
      "source": [
        "fdist1.freq('monstrous')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "En96R2GHX0sT"
      },
      "outputs": [],
      "source": [
        "fdist1.N()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5DT6qSvqX0sT"
      },
      "outputs": [],
      "source": [
        "fdist1.tabulate(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iopkUBxRX0sU"
      },
      "outputs": [],
      "source": [
        "fdist1.plot(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fyLSasUKX0sU"
      },
      "outputs": [],
      "source": [
        "fdist1.plot(10,cumulative=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_uUvHhGyX0sU"
      },
      "outputs": [],
      "source": [
        "fdist5 = FreqDist(text5)\n",
        "sorted(w for w in set(text5) if len(w) > 7 and fdist5[w] > 7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jrNQYAqKX0sU"
      },
      "outputs": [],
      "source": [
        "list1=['more', 'is', 'said', 'than', 'done']\n",
        "[w.upper() for w in list1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WE2SMVkkX0sV"
      },
      "source": [
        "#### Collocations \n",
        "Collocations are two or more words that tend to appear frequently together, for example – United States. There are many other words that can come after United, such as the United Kingdom and United Airlines. As with many aspects of natural language processing, context is very important. And for collocations, context is everything."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UcK6I-ugX0sV"
      },
      "outputs": [],
      "source": [
        "from nltk.collocations import BigramCollocationFinder\n",
        "from nltk.metrics import BigramAssocMeasures\n",
        "biagram_collocation=BigramCollocationFinder.from_words(text2)\n",
        "\n",
        "biagram_collocation.nbest(BigramAssocMeasures.likelihood_ratio, 20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xB23NzoHX0sW"
      },
      "outputs": [],
      "source": [
        "from nltk.collocations import TrigramCollocationFinder\n",
        "from nltk.metrics import TrigramAssocMeasures\n",
        "trigram_collocation = TrigramCollocationFinder.from_words(text2)\n",
        "trigram_collocation.nbest(TrigramAssocMeasures.likelihood_ratio, 15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EgFLt64nX0sW"
      },
      "outputs": [],
      "source": [
        "input_text = \"Game of Thrones is an American fantasy drama television series \\\n",
        "created by David Benioff and D. B. Weiss for HBO. It is an adaptation of A Song \\\n",
        "of Ice and Fire, George R. R. Martin's series of fantasy novels, the first of \\\n",
        "which is A Game of Thrones. The show was filmed in Belfast and elsewhere in the \\\n",
        "United Kingdom, Canada, Croatia, Iceland, Malta, Morocco, Spain, and the \\\n",
        "United States.[1] The series premiered on HBO in the United States on April \\\n",
        "17, 2011, and concluded on May 19, 2019, with 73 episodes broadcast over \\\n",
        "eight seasons. Set on the fictional continents of Westeros and Essos, Game of \\\n",
        "Thrones has several plots and a large ensemble cast, and follows several story \\\n",
        "arcs. One arc is about the Iron Throne of the Seven Kingdoms, and follows a web \\\n",
        "of alliances and conflicts among the noble dynasties either vying to claim the \\\n",
        "throne or fighting for independence from it. Another focuses on the last \\\n",
        "descendant of the realm's deposed ruling dynasty, who has been exiled and is \\\n",
        "plotting a return to the throne, while another story arc follows the Night's \\\n",
        "Watch, a brotherhood defending the realm against the fierce peoples and \\\n",
        "legendary creatures of the North.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jsWtvJheX0sW"
      },
      "outputs": [],
      "source": [
        "text = nltk.Text( input_text.split() )\n",
        "from nltk.collocations import BigramCollocationFinder\n",
        "from nltk.metrics import BigramAssocMeasures\n",
        "biagram_collocation=BigramCollocationFinder.from_words(text)\n",
        "\n",
        "biagram_collocation.nbest(BigramAssocMeasures.likelihood_ratio, 5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0uO_fC79X0sX"
      },
      "outputs": [],
      "source": [
        "from nltk.collocations import TrigramCollocationFinder\n",
        "from nltk.metrics import TrigramAssocMeasures\n",
        "trigram_collocation = TrigramCollocationFinder.from_words(text)\n",
        "trigram_collocation.nbest(TrigramAssocMeasures.likelihood_ratio, 15)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_g5OZxNX0sX"
      },
      "source": [
        "# Accessing Text Corpora\n",
        "\n",
        "###### Gutenberg Corpus\n",
        "###### Web and Chat Text Corpus\n",
        "###### Brown Corpus\n",
        "###### Reuters Corpus \n",
        "###### Inaugural Address Corpus \n",
        "###### Loading your own Corpus "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PL-RqO0gX0sX"
      },
      "source": [
        "##### Gutenberg Corpus\n",
        "\n",
        "NLTK includes a small selection of texts from the Project Gutenberg electronic text archive, which contains some 25,000 free electronic books, hosted at http://www.gutenberg.org/. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ycaE5UwbX0sY"
      },
      "outputs": [],
      "source": [
        "gutenberg.fileids()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Zehyvn-X0sY"
      },
      "outputs": [],
      "source": [
        "len(gutenberg.words('austen-emma.txt'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3bFYYOXCX0sY"
      },
      "outputs": [],
      "source": [
        "len(gutenberg.sents('austen-emma.txt'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MEPnow-zX0sZ"
      },
      "outputs": [],
      "source": [
        "len(gutenberg.raw('austen-emma.txt'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mIq8dRF0X0sZ"
      },
      "outputs": [],
      "source": [
        "for fileid in gutenberg.fileids():\n",
        "    num_chars = len(gutenberg.raw(fileid))\n",
        "    num_words = len(gutenberg.words(fileid))\n",
        "    num_sents = len(gutenberg.sents(fileid))\n",
        "    num_vocab = len(set(w.lower() for w in gutenberg.words(fileid)))\n",
        "    print(round(num_chars/num_words))\n",
        "    print(round(num_words/num_sents))\n",
        "    print(round(num_words/num_vocab))\n",
        "    print(fileid)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GmA1WtEEX0sa"
      },
      "outputs": [],
      "source": [
        "# collocation\n",
        "\n",
        "text2=gutenberg.words('austen-emma.txt')\n",
        "\n",
        "biagram_collocation=BigramCollocationFinder.from_words(text2)\n",
        "\n",
        "biagram_collocation.nbest(BigramAssocMeasures.likelihood_ratio, 20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hdZJMEhEX0sa"
      },
      "source": [
        "Apply all the above discussed methods like lexial diversity, Concordance....."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQauz3cpX0sa"
      },
      "source": [
        "##### Web and Chat Text\n",
        "\n",
        "NLTK's small collection of web text includes content from a Firefox discussion forum, conversations overheard in New York, the movie script of Pirates of the Carribean, personal advertisements, and wine reviews:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LoVW4mWOX0sa"
      },
      "outputs": [],
      "source": [
        "# from nltk.corpus import webtext\n",
        "webtext.fileids()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_SFHWGUcX0sb"
      },
      "outputs": [],
      "source": [
        "nps_chat.fileids()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNz9NZUHX0sb"
      },
      "source": [
        "##### Brown Corpus\n",
        "The Brown Corpus was the first million-word electronic corpus of English, created in 1961 at Brown University. This corpus contains text from 500 sources, and the sources have been categorized by genre, such as news, editorial, and so on. 1.1 gives an example of each genre (for a complete list, see http://icame.uib.no/brown/bcm-los.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TntSp0WEX0sb"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import brown\n",
        "brown.categories()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n8VizmxbX0sc"
      },
      "outputs": [],
      "source": [
        " brown.words(categories='news')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "06f1cASSX0sc"
      },
      "outputs": [],
      "source": [
        "brown.words(fileids=['cg22'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vQlXKpbzX0sc"
      },
      "outputs": [],
      "source": [
        " brown.sents(categories=['news', 'editorial', 'reviews'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SHo9Sxn1X0sc"
      },
      "outputs": [],
      "source": [
        "# Program 1\n",
        "\n",
        "from nltk.corpus import brown\n",
        "news_text = brown.words(categories='news')\n",
        "fdist = nltk.FreqDist(w.lower() for w in news_text)\n",
        "modals = ['can', 'could', 'may', 'might', 'must', 'will']\n",
        "for m in modals:\n",
        "    print(m + ':', fdist[m])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WK41HFmNX0sd"
      },
      "outputs": [],
      "source": [
        "# Brown Corpus  and CFD (Program 2)\n",
        "\n",
        "\n",
        "cfd = nltk.ConditionalFreqDist((genre, word) \n",
        "                               for genre in brown.categories() \n",
        "                               for word in brown.words(categories=genre))\n",
        "genres = ['news', 'religion', 'hobbies', 'science_fiction', 'romance', 'humor']\n",
        "modals = ['can', 'could', 'may', 'might', 'must', 'will']\n",
        "cfd.tabulate(conditions=genres, samples=modals)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FJ28HLKJX0sd"
      },
      "outputs": [],
      "source": [
        "cfd.plot(conditions=genres, samples=modals) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zcephVx5X0sd"
      },
      "outputs": [],
      "source": [
        "# Reuters Corpus (HW)\n",
        "# Inaugural Address Corpus (HW)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55CIRZJ-X0sd"
      },
      "source": [
        "#### Create your own Corpus "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Kou4VKDX0se"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import PlaintextCorpusReader\n",
        "corpus_root = 'C://Users//Nitin Arvind Shelke//Desktop//abc'\n",
        "owncorpus = PlaintextCorpusReader(corpus_root, '.*')\n",
        "print(owncorpus.fileids())\n",
        "print(len(owncorpus.fileids()))\n",
        "print(owncorpus.sents(fileids='DFS.txt'))\n",
        "print(owncorpus.words(fileids='DFS.txt'))\n",
        "print(len(owncorpus.words(fileids='DFS.txt')))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CgVqZh-DX0se"
      },
      "source": [
        "Apply all the above discussed NLP operations on self created coprpus (HW)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZVCEvd8X0se"
      },
      "source": [
        "# Extraction of Text Data from PDF file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUPMd-zbX0sf"
      },
      "source": [
        "##### Extraction of Text Data from single page pdf file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fGkCqEbAX0sf"
      },
      "outputs": [],
      "source": [
        "\n",
        "import PyPDF2\n",
        "path = 'C:\\\\Users\\\\Nitin Arvind Shelke\\\\Desktop\\\\abc\\\\tod.pdf'\n",
        "\n",
        "# reading from single page\n",
        "read_pdf = PyPDF2.PdfFileReader(path)\n",
        "page = read_pdf.getPage(0)\n",
        "page_content = page.extractText()\n",
        "print (page_content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t2UAYuNmX0sf"
      },
      "outputs": [],
      "source": [
        "# Reading and printing multiple pages \n",
        "path = 'C:\\\\Users\\\\Nitin Arvind Shelke\\\\Desktop\\\\abc\\\\tod2.pdf'\n",
        "\n",
        "# reading from single page\n",
        "read_pdf = PyPDF2.PdfFileReader(path)\n",
        "\n",
        "for i in range(read_pdf.getNumPages()):\n",
        "    page = read_pdf.getPage(i)\n",
        "    print ('Page No - ' + str(1+read_pdf.getPageNumber(page)))\n",
        "    page_content = page.extractText()\n",
        "    print (page_content)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NuoashFPX0sg"
      },
      "source": [
        "# Lexical Resources\n",
        "\n",
        "\n",
        "###### Wordlist Corpora\n",
        "###### Name Corpus\n",
        "###### Comparative Wordlists\n",
        "###### WordNet\n",
        "###### Stopword\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vx_vz_V6X0sg"
      },
      "source": [
        "### Wordlist Corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sstau1dXX0sg"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import words\n",
        "print(words.fileids())\n",
        "print(words.words('en')[1:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AD7ybQv1X0sh"
      },
      "source": [
        " Write a Code for Filtering a Text that computes the vocabulary of a text, then removes all items that occur in an existing wordlist, leaving just the uncommon or misspelled words.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6g1k5LAsX0sh"
      },
      "outputs": [],
      "source": [
        "def unusual_words(text):\n",
        "    t_vocab = set(w.lower() for w in text if w.isalpha())\n",
        "    e_vocab = set(w.lower() for w in nltk.corpus.words.words())\n",
        "    unusual = t_vocab - e_vocab\n",
        "    return sorted(unusual)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "em20QkUfX0sh"
      },
      "outputs": [],
      "source": [
        "unusual_words(nltk.corpus.reuters.words( ))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhWrDi4RX0sh"
      },
      "source": [
        "Puzzle code : A wordlist is useful for solving word puzzles. How many words of six letters or more can you make from those shown (agivrnlpe)? With condition that each letter must be used once only and must contain  center letter\n",
        "\n",
        "\n",
        "![image.png](attachment:image.png)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N1XlcfdZX0si"
      },
      "outputs": [],
      "source": [
        "puzzle_letters = nltk.FreqDist('agivrnlpe')\n",
        "compulsory = 'r'\n",
        "wordlist = nltk.corpus.words.words()\n",
        "[w for w in wordlist \n",
        " if len(w) >= 6 and compulsory in w \n",
        " and nltk.FreqDist(w) <= puzzle_letters]\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vp8-Q_-3X0si"
      },
      "source": [
        "#HW\n",
        "\n",
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GoJwozCBX0si"
      },
      "source": [
        "###### Name Corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P7ilzEZSX0sj"
      },
      "outputs": [],
      "source": [
        "names = nltk.corpus.names\n",
        "names.fileids()\n",
        "male_names = names.words('male.txt')\n",
        "female_names = names.words('female.txt')\n",
        "print([w for w in male_names if w in female_names])\n",
        "\n",
        "print([w for w in male_names if w in male_names])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rt6zy-OLX0sj"
      },
      "outputs": [],
      "source": [
        "#Draw a plot that shows the number of female and male names ending with each letter of the alphabet.\n",
        "\n",
        "cfd = nltk.ConditionalFreqDist((fileid, name[-1])\n",
        "                               for fileid in names.fileids()\n",
        "                               for name in names.words(fileid))\n",
        "cfd.plot()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSwIjSRLX0sj"
      },
      "source": [
        "###### Comparative Wordlists"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "seWmw30dX0sk"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import swadesh\n",
        "swadesh.fileids()\n",
        "swadesh.words('en')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "soBgq1xNX0sl"
      },
      "outputs": [],
      "source": [
        "# entries() method\n",
        "# We can access cognate words from multiple languages using the entries() method, specifying a list of languages. \n",
        "fr2en = swadesh.entries(['fr', 'en'])\n",
        "#fr2en\n",
        "translate = dict(fr2en)\n",
        "print(translate['chien'])\n",
        "print(translate['jeter'])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54rQLIvkX0sl"
      },
      "source": [
        "### WordNet:\n",
        "\n",
        "WordNet is a huge collection of words with meanings just like a traditional dictionary, used to generate synonyms, antonyms of words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GtyNjt0mX0sm"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import wordnet\n",
        "syn = wordnet.synsets(\"pain\")\n",
        "print(syn)\n",
        "print(syn[0].lemma_names())\n",
        "print(syn[0].definition())\n",
        "print(syn[0].examples())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uja-v4I2X0sm"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import wordnet\n",
        "synonym = wordnet.synsets(\"AI\")\n",
        "print(synonym)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5rTgQEsTX0sn"
      },
      "outputs": [],
      "source": [
        "# Print definition using WordNet:\n",
        "print(synonym[1].definition())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v_gShnjEX0sn"
      },
      "outputs": [],
      "source": [
        "# Print examples using WordNet:\n",
        "print(synonym[1].examples())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7kmQAlHuX0sn"
      },
      "outputs": [],
      "source": [
        "syn=wordnet.synsets('motorcar')\n",
        "print(syn)\n",
        "print(wordnet.synset('car.n.01').lemma_names())\n",
        "print(wordnet.synset('car.n.02').lemma_names())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aFk4ZUuzX0so"
      },
      "outputs": [],
      "source": [
        "syn=wordnet.synsets('car')\n",
        "print(syn)\n",
        "for syn in wordnet.synsets('car'):\n",
        "    print(syn.lemma_names())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRoHZAkAX0so"
      },
      "source": [
        "compare the similarity index of any two words\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qFpWgKUZX0sp"
      },
      "outputs": [],
      "source": [
        "w1 = wordnet.synset('run.v.01') # v here denotes the tag verb \n",
        "w2 = wordnet.synset('sprint.v.01') \n",
        "print(w1.wup_similarity(w2)) \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DLQ3g0-kX0sq"
      },
      "outputs": [],
      "source": [
        "# Let's compare the noun of \"ship\" and \"boat:\"  ‘car’ and ‘motorcar’\n",
        "w1 = wordnet.synset('ship.v.01') # v here denotes the tag verb \n",
        "w2 = wordnet.synset('boat.v.01') \n",
        "print(w1.wup_similarity(w2)) \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gH4P52i4X0sr"
      },
      "outputs": [],
      "source": [
        "# Let's compare the noun of \"ship\" and \"boat:\"  ‘car’ and ‘motorcar’\n",
        "w1 = wordnet.synset('car.n.01') # v here denotes the tag verb \n",
        "w2 = wordnet.synset('motorcar.n.01') \n",
        "print(w1.wup_similarity(w2)) \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05hFDWMtX0sr"
      },
      "source": [
        "###### Stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C4TyQ_eWX0sr"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import stopwords\n",
        "stopwords.words('english')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXyy-T7yX0ss"
      },
      "source": [
        " WAP to define a function that filter the words in a text are not in the stopwords list (remove the stopwords from the text file)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ta78ytsQX0ss"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "def filter_content(text):\n",
        "    stopwords = set(nltk.corpus.stopwords.words('english'))\n",
        "    filter_content = [w for w in text if w.lower() not in stopwords]\n",
        "    return filter_content\n",
        "\n",
        "print(filter_content(nltk.corpus.gutenberg.words('austen-emma.txt')))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZUgTzbqNX0ss"
      },
      "source": [
        "WAP to define a function that compute what fraction of words in a text are not in the stopwords list:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xx3884S7X0ss"
      },
      "outputs": [],
      "source": [
        "def content_fraction(text):\n",
        "    stopwords = nltk.corpus.stopwords.words('english')\n",
        "    content = [w for w in text if w.lower() not in stopwords]\n",
        "    return len(content) / len(text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KnPvtImtX0st"
      },
      "outputs": [],
      "source": [
        "#To call function :\n",
        "content_fraction(nltk.corpus.gutenberg.words())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cIi1hXD_X0st"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MvPrDk_7X0st"
      },
      "source": [
        "##### Case Study"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MRg9PeoiX0st"
      },
      "outputs": [],
      "source": [
        "input_text = \"Game of Thrones is an American fantasy drama television series \\\n",
        "created by David Benioff and D. B. Weiss for HBO. It is an adaptation of A Song \\\n",
        "of Ice and Fire, George R. R. Martin's series of fantasy novels, the first of \\\n",
        "which is A Game of Thrones. The show was filmed in Belfast and elsewhere in the \\\n",
        "United Kingdom, Canada, Croatia, Iceland, Malta, Morocco, Spain, and the \\\n",
        "United States.[1] The series premiered on HBO in the United States on April \\\n",
        "17, 2011, and concluded on May 19, 2019, with 73 episodes broadcast over \\\n",
        "eight seasons. Set on the fictional continents of Westeros and Essos, Game of \\\n",
        "Thrones has several plots and a large ensemble cast, and follows several story \\\n",
        "arcs. One arc is about the Iron Throne of the Seven Kingdoms, and follows a web \\\n",
        "of alliances and conflicts among the noble dynasties either vying to claim the \\\n",
        "throne or fighting for independence from it. Another focuses on the last \\\n",
        "descendant of the realm's deposed ruling dynasty, who has been exiled and is \\\n",
        "plotting a return to the throne, while another story arc follows the Night's \\\n",
        "Watch, a brotherhood defending the realm against the fierce peoples and \\\n",
        "legendary creatures of the North.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dqJIm3KjX0su"
      },
      "outputs": [],
      "source": [
        "type(input_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XLxSWglEX0su"
      },
      "outputs": [],
      "source": [
        "text = nltk.Text( input_text.split() )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "udW4HeGhX0su"
      },
      "outputs": [],
      "source": [
        "text.concordance('in')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WtnHUoRNX0sv"
      },
      "outputs": [],
      "source": [
        "text.similar('show ')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fnw3o5clX0sv"
      },
      "outputs": [],
      "source": [
        "text.common_contexts(['game', 'web'])  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "34qC8M6zX0sx"
      },
      "outputs": [],
      "source": [
        "from nltk import FreqDist\n",
        "fdist=FreqDist(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2c-NJ41nX0sx"
      },
      "outputs": [],
      "source": [
        "fdist.most_common()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "scXthzqEX0sx"
      },
      "outputs": [],
      "source": [
        "fdist.tabulate(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6hCKrtSX0sy"
      },
      "source": [
        "##### Remove the stopowords from the input_text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YLYlAsBTX0sy"
      },
      "outputs": [],
      "source": [
        "def filter_content(text):\n",
        "    stopwords = set(nltk.corpus.stopwords.words('english'))\n",
        "    filter_content = [w for w in text if w.lower() not in stopwords]\n",
        "    fraction= len(filter_content) / len(text)\n",
        "    return filter_content,fraction\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XfqUa4MhX0sy"
      },
      "outputs": [],
      "source": [
        "filter_content, fraction=filter_content(text)   \n",
        "print(filter_content)\n",
        "print(fraction)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "NLP Day 1 Hands on.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}