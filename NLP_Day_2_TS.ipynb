{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NitinAShelke/NLP-with-NLTK-Thapar-Summer-School/blob/main/NLP_Day_2_TS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZEsAFCveizr"
      },
      "source": [
        "# Hands on NLTK\n",
        "\n",
        "#### by Nitin A. Shelke"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "313v05Qseizv"
      },
      "source": [
        "## Topics to be covered"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWy5A8xneizw"
      },
      "source": [
        "\n",
        "#### Lowercasing\n",
        "#### Removing Stop words\n",
        "#### Tokenization\n",
        "#### Stemming \n",
        "#### Lemmatization\n",
        "#### Remove punctuation\n",
        "#### Remove numbers\n",
        "#### Spell Check\n",
        "#### Case study on Preprocessing\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWIA0HOseizx"
      },
      "source": [
        "## Import NLTK Library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RtZkoLJGeizz"
      },
      "outputs": [],
      "source": [
        "import nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WS5TkxTHeiz0"
      },
      "source": [
        "## lowercasing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FU48QEGXeiz0"
      },
      "outputs": [],
      "source": [
        "str = \"Welcome TO THE World of Python\"\n",
        "str.lower()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mpnGVoC_eiz1"
      },
      "outputs": [],
      "source": [
        "#Gutenberg Corpus\n",
        "from nltk.corpus import gutenberg as gt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BuGp4AMeeiz1"
      },
      "outputs": [],
      "source": [
        "gt.fileids()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eag71wnieiz2"
      },
      "outputs": [],
      "source": [
        "len(gt.words('austen-emma.txt')) # words "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VjrtRG-peiz2"
      },
      "outputs": [],
      "source": [
        "len(gt.sents('austen-emma.txt')) # sents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TMHGibLBeiz3"
      },
      "outputs": [],
      "source": [
        "len(gt.raw('austen-emma.txt')) # charcter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FmpSbTT7eiz3"
      },
      "source": [
        "## Stoword Removal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zjKG17Gleiz3"
      },
      "outputs": [],
      "source": [
        "#to import the stopword corpus\n",
        "from nltk.corpus import stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "__gaYFRWeiz4"
      },
      "outputs": [],
      "source": [
        "#check diffrent language stopoword\n",
        "stopwords.fileids()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oxf88Hqeeiz4"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PCas2f9Weiz4"
      },
      "outputs": [],
      "source": [
        "#English Language Stopword\n",
        "stopword=stopwords.words('english')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z3NSiJkWeiz4"
      },
      "outputs": [],
      "source": [
        "actual_word=gt.words('austen-emma.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mx_bkiqqeiz5"
      },
      "outputs": [],
      "source": [
        "#Stopword Example 1\n",
        "#Code to define a function that filter the words in a text are not in the stopwords list \n",
        "#(remove the stopwords from the text file)\n",
        "def filter_content(text):\n",
        "    filter_content = [w for w in text if w.lower() not in stopword]\n",
        "    return filter_content\n",
        "\n",
        "#print(filter_content(nltk.corpus.gutenberg.words('austen-emma.txt')))\n",
        "\n",
        "print(filter_content(actual_word))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Ilqsvhceiz5"
      },
      "outputs": [],
      "source": [
        "#WAP to define a function that compute what fraction of words in a text are not in the stopwords list\n",
        "\n",
        "def content_fraction(text):\n",
        "    content = [w for w in text if w.lower() not in stopwords.words('english')]\n",
        "    return len(content) / len(text)*100\n",
        "\n",
        "filter_content=content_fraction(nltk.corpus.gutenberg.words('austen-emma.txt'))\n",
        "print(filter_content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DrmgJYfveiz5"
      },
      "outputs": [],
      "source": [
        "input_text = \"Game of Thrones is an American fantasy drama television series created by David Benioff and D. B. Weiss for HBO. It is an adaptation of A Song  of Ice and Fire, George R. R. Martin's series of fantasy novels, the first of which is A Game of Thrones. The show was filmed in Belfast and elsewhere in the  United Kingdom, Canada, Croatia, Iceland, Malta, Morocco, Spain, and the United States. The series premiered on HBO in the United States on April  17, 2011, and concluded on May 19, 2019, with 73 episodes broadcast over eight seasons. Set on the fictional continents of Westeros and Essos, Game of Thrones has several plots and a large ensemble cast, and follows several story arcs. One arc is about the Iron Throne of the Seven Kingdoms, and follows a web of alliances and conflicts among the noble dynasties either vying to claim the throne or fighting for independence from it. Another focuses on the last descendant of the realm's deposed ruling dynasty, who has been exiled and is plotting a return to the throne, while another story arc follows the Night's Watch, a brotherhood defending the realm against the fierce peoples and legendary creatures of the North.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-EKPSmbMeiz6"
      },
      "outputs": [],
      "source": [
        "type(input_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KFqX2ROseiz6"
      },
      "outputs": [],
      "source": [
        "text = nltk.Text( input_text.split() )\n",
        "type(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "37LcsIqKeiz6"
      },
      "outputs": [],
      "source": [
        "def filter_content(text):\n",
        "    filter_content = [w for w in text if w.lower() not in stopwords.words('english')]\n",
        "    fraction= len(filter_content) / len(text)*100\n",
        "    return filter_content,fraction\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m9xVnkKgeiz7"
      },
      "outputs": [],
      "source": [
        "filter_content, fraction=filter_content(text)   \n",
        "print(filter_content)\n",
        "print(fraction)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpVUGDkIeiz7"
      },
      "source": [
        "## Tokenization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gnk5kyJ1eiz7"
      },
      "outputs": [],
      "source": [
        "#Sentence Tokenization:\n",
        "str=\"Hello Students Python. This is NLTK.\"\n",
        "from nltk.tokenize import sent_tokenize\n",
        "sent_tokenize(str)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q30IOLgheiz7"
      },
      "outputs": [],
      "source": [
        "input_text = \"Game of Thrones is an American fantasy drama television series created by David Benioff and D. B. Weiss for HBO. It is an adaptation of A Song  of Ice and Fire, George R. R. Martin's series of fantasy novels, the first of which is A Game of Thrones. The show was filmed in Belfast and elsewhere in the  United Kingdom, Canada, Croatia, Iceland, Malta, Morocco, Spain, and the United States. The series premiered on HBO in the United States on April  17, 2011, and concluded on May 19, 2019, with 73 episodes broadcast over eight seasons. Set on the fictional continents of Westeros and Essos, Game of Thrones has several plots and a large ensemble cast, and follows several story arcs. One arc is about the Iron Throne of the Seven Kingdoms, and follows a web of alliances and conflicts among the noble dynasties either vying to claim the throne or fighting for independence from it. Another focuses on the last descendant of the realm's deposed ruling dynasty, who has been exiled and is plotting a return to the throne, while another story arc follows the Night's Watch, a brotherhood defending the realm against the fierce peoples and legendary creatures of the North.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NCAHYW4Ceiz7"
      },
      "outputs": [],
      "source": [
        "len(sent_tokenize(input_text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cN900wGPeiz7"
      },
      "outputs": [],
      "source": [
        "str=\"Hello Students Python. This is NLTK.\"\n",
        "from nltk.tokenize import word_tokenize\n",
        "len(word_tokenize(str))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bhRs9n4Aeiz8"
      },
      "outputs": [],
      "source": [
        "#Word Tokenization:\n",
        "from nltk.tokenize import word_tokenize\n",
        "word_tokenize(\"This is NLTK.\")\n",
        "print(len(set(word_tokenize(\"This is NLTK.\"))))\n",
        "print(len(set(word_tokenize(\"This is NLTK. This is Python\"))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vFnRy9O2eiz8"
      },
      "outputs": [],
      "source": [
        "#Capturing User Input \n",
        "s = input(\"Enter some text: \")\n",
        "print(nltk.word_tokenize(s))\n",
        "len(nltk.word_tokenize(s))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uc1kcM7Ueiz8"
      },
      "outputs": [],
      "source": [
        "#Capturing User Input \n",
        "s = input(\"Enter some text: \")\n",
        "print(nltk.sent_tokenize(s))\n",
        "len(nltk.sent_tokenize(s))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "toh5G1t6eiz8"
      },
      "outputs": [],
      "source": [
        "#To Remove the punctuation and numbers\n",
        "\n",
        "s = \"I can do this now @, because 12 I am so tired.  Please 98 give me some time. @ sd  4 232\"\n",
        "words = nltk.word_tokenize(s)\n",
        "words=[word.lower() for word in words if word.isalpha()]\n",
        "print(words)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yJSqKZCTeiz8"
      },
      "outputs": [],
      "source": [
        "#Code to tokenize the input sentence and remove the stop words\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "input_txt = \"Aman is a one guy from the india who takes care of people around him.\"\n",
        "tokens = word_tokenize(input_txt)\n",
        "print('Number of tokens in input_txt : ',len(tokens))\n",
        "filt_sent = [w for w in tokens if not w in stopwords.words('english')]\n",
        "print(filt_sent)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJApvXaUeiz8"
      },
      "source": [
        "## Stemming"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2jggqdAeeiz9"
      },
      "outputs": [],
      "source": [
        "#create the object of porter stemmer class\n",
        "ps=nltk.PorterStemmer()\n",
        "print(ps.stem('playing'))\n",
        "print(ps.stem('plays'))\n",
        "print(ps.stem('cats'))\n",
        "print(ps.stem('cars'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qiMkRYf_eiz9"
      },
      "outputs": [],
      "source": [
        "#Example PorterStemmer\n",
        "\n",
        "#create the object of porter stemmer class\n",
        "ps=nltk.PorterStemmer()\n",
        "\n",
        "# choose some words to be stemmed \n",
        "words = [\"program\", \"programs\", \"programer\", \"programing\", \"programers\"] \n",
        "  \n",
        "for w in words: \n",
        "    print(w, \" : \", ps.stem(w)) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MA2tVAJVeiz9"
      },
      "outputs": [],
      "source": [
        "#Exmple PorterStemmer\n",
        "\n",
        "new_text = \"It is important to by high pythoning while you are pythoning with python. All pythoners have pythoned poor at least.\"\n",
        "\n",
        "word_tokens = nltk.word_tokenize(new_text)\n",
        "for w in word_tokens:\n",
        "    print(ps.stem(w))   # Passing word tokens into stem method of Porter Stemmer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ytSNDzEgeiz9"
      },
      "outputs": [],
      "source": [
        "#Example LancasterStemmer\n",
        "ls=nltk.LancasterStemmer() # create \n",
        "print(ps.stem('strange'))\n",
        "print(ps.stem('plays'))\n",
        "print(ps.stem('cats'))\n",
        "print(ps.stem('cars'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ziN4plIWeiz9"
      },
      "outputs": [],
      "source": [
        "#Example LancasterStemmer\n",
        "words = [\"playing\",\"eating\",\"played\",\"writing\"]\n",
        "ps = nltk.PorterStemmer()\n",
        "for word in words:\n",
        "    print(ps.stem(word))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QUAfxRTeeiz9"
      },
      "outputs": [],
      "source": [
        "#Example PorterStemmer vs LancasterStemmer\n",
        "words = [\"playing\",\"eating\",\"played\",\"writing\"]\n",
        "ps = nltk.PorterStemmer()\n",
        "ls = nltk.LancasterStemmer()\n",
        "for word in words:\n",
        "    print(ps.stem(word)) or print(ls.stem(word))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_vHVREBmeiz-"
      },
      "outputs": [],
      "source": [
        "#Example PorterStemmer vs LancasterStemmer\n",
        "raw = \"DENNIS: Listen, strange women lying in ponds distributing swords is no basis for a system of government. Supreme executive power derives from a mandate from the masses, not from some farcical aquatic ceremony.\"\n",
        "tokens = nltk.word_tokenize(raw)\n",
        "porter = nltk.PorterStemmer()\n",
        "lancaster = nltk.LancasterStemmer()\n",
        "print([porter.stem(t) for t in tokens])\n",
        "print([lancaster.stem(t) for t in tokens])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JLz3HzeEeiz-"
      },
      "source": [
        "## Lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uCbuMDBGeiz-"
      },
      "outputs": [],
      "source": [
        "lemmatizer = nltk.WordNetLemmatizer()\n",
        "print(lemmatizer.lemmatize('increases'))\n",
        "print(lemmatizer.lemmatize('stones')) \n",
        "print(lemmatizer.lemmatize('speaking'))\n",
        "print(lemmatizer.lemmatize('bedroom'))\n",
        "print(lemmatizer.lemmatize('jokes'))\n",
        "print(lemmatizer.lemmatize('strange'))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMs8uLkveiz-"
      },
      "source": [
        "## Spell Check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4sz-HNrEeiz-"
      },
      "outputs": [],
      "source": [
        "#!pip install autocorrect\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nsdp7tpUeiz-"
      },
      "outputs": [],
      "source": [
        "#Spell Check example\n",
        "\n",
        "from autocorrect import Speller\n",
        "spell = Speller(lang='en')\n",
        "text = \"This is a wrld of hope\"\n",
        "text1=nltk.word_tokenize(text)\n",
        "spells = [spell(w) for w in (text1)]\n",
        "print (spells)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0lhgV5Jreiz-"
      },
      "source": [
        "## Regular Expressions for Detecting Word Patterns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xBy0xwv_eiz-"
      },
      "outputs": [],
      "source": [
        "wsj = sorted(set(nltk.corpus.treebank.words()))\n",
        "[w for w in wsj if re.search('^[0-9]+\\.[0-9]+$', w)]\n",
        "[w for w in wsj if re.search('^[A-Z]+\\$$', w)]\n",
        "[w for w in wsj if re.search('^[0-9]{4}$', w)]\n",
        "[w for w in wsj if re.search('^[a-z]{5,}-[a-z]{2,3}-[a-z]{,6}$', w)]\n",
        "[w for w in wsj if re.search('(ed|ing)$', w)]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-nvEPQq0eiz_"
      },
      "source": [
        "## Regular Expressions for Extracting Word Pieces"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fsZJw0i8eiz_"
      },
      "outputs": [],
      "source": [
        "word = 'supercalifragilisticexpialidocious12345'\n",
        "re.findall(r'[aeiou]', word)\n",
        "re.findall(r'[0-9]', word)\n",
        "re.findall(r'i*', word)\n",
        "re.findall(r'i+', word)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMA_YtOjeiz_"
      },
      "source": [
        "# Regular Expressions for Stemming"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q20jhsaqeiz_"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import re\n",
        "def stem(word):\n",
        "    regexp = r'^(.*?)(ing|ly|ed|ious|ies|ive|es|s|ment)?$'\n",
        "    stem, suffix = re.findall(regexp, word)[0]\n",
        "    return stem\n",
        "raw = \"\"\"DENNIS: Listen, strange women lying in ponds distributing swords\n",
        "... is no basis for a system of government. Supreme executive power derives from\n",
        "... a mandate from the masses, not from some farcical aquatic ceremony.\"\"\"\n",
        "tokens = nltk.word_tokenize(raw)\n",
        "[stem(t) for t in tokens]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oPPMnqUheiz_"
      },
      "source": [
        "## Remove punctuation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qiJkby-peiz_"
      },
      "outputs": [],
      "source": [
        "from string import punctuation\n",
        "\n",
        "\n",
        "def remove_punctuation(s):\n",
        "    return ''.join(c for c in s if c not in punctuation)\n",
        "\n",
        "\n",
        "text = \"Hello! how are you doing?\"\n",
        "print (remove_punctuation(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AUGjxhU5ei0A"
      },
      "outputs": [],
      "source": [
        "#example2\n",
        "s = \"I can do this now @, because 12 I am so tired.  Please 98 give me some time. @ sd  4 232\"\n",
        "print (remove_punctuation(s))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mBG2-592ei0A"
      },
      "source": [
        "## Remove Numbers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qekBOXLdei0A"
      },
      "outputs": [],
      "source": [
        "#To remove numbers, you can use: .isnumeric() else .isdigit() \n",
        "text = \"There was 200 people standing right next to me at 2pm.\"\n",
        "output = ''.join(c for c in text if not c.isdigit())\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S2-P9e3zei0A"
      },
      "source": [
        "## Part-of-Speech Tagging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xqdRjxggei0A"
      },
      "outputs": [],
      "source": [
        "text = \"I'm going to meet M.S. Dhoni.\"\n",
        "tokenized_word = nltk.word_tokenize(text)\n",
        "nltk.pos_tag(tokenized_word, tagset='universal')\n",
        "text = \"I'm going to meet M.S. Dhoni.\"\n",
        "tokenized_word = nltk.word_tokenize(text)\n",
        "nltk.pos_tag(tokenized_word, tagset='universal')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "elw2ziVbei0A"
      },
      "source": [
        "## Named Entity Recognition:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xTT-uQH2ei0A"
      },
      "outputs": [],
      "source": [
        "text = \"Sundar Pichai, the CEO of Google Inc. is walking in the streets of California.\"\n",
        "tokenized_word =nltk.word_tokenize(text)\n",
        "tags = nltk.pos_tag(tokenized_word, tagset='universal')\n",
        "entities = nltk.chunk.ne_chunk(tags, binary=False)\n",
        "print(entities)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ruIlzXFxei0B"
      },
      "source": [
        "## Open the File and read it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oCmuJ4KYei0B"
      },
      "outputs": [],
      "source": [
        "f = open('pride.txt', encoding=\"utf8\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "llwaMIpXei0B"
      },
      "outputs": [],
      "source": [
        "text = f.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0IhLrB1qei0B"
      },
      "outputs": [],
      "source": [
        "print(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qTnbchmlei0B"
      },
      "source": [
        "## Case Study of Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AImQnaJ8ei0B"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import string\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from string import punctuation\n",
        "\n",
        "def preprocess(filename):\n",
        "    f = open(filename,'r',encoding=\"utf8\")\n",
        "    text = f.read()\n",
        "    \n",
        "    text = text.lower()\n",
        "    \n",
        "    text_p = \"\".join([char for char in text if char not in string.punctuation])\n",
        "    \n",
        "    text_rn = \"\".join([char for char in text if not char.isdigit()])\n",
        "    \n",
        "    \n",
        "    #''.join(c for c in text if not c.isdigit())\n",
        "    \n",
        "    words = word_tokenize(text_p)\n",
        "    \n",
        "    stop_words = stopwords.words('english')\n",
        "    filtered_words = [word for word in words if word not in stop_words]\n",
        "    \n",
        "    porter = PorterStemmer()\n",
        "    port_stemmed = [porter.stem(word) for word in filtered_words]\n",
        "    \n",
        "    lancaster = nltk.LancasterStemmer()\n",
        "    lancat_stemmed = [lancaster.stem(word) for word in filtered_words]\n",
        "    \n",
        "    lemmatizer = nltk.WordNetLemmatizer()\n",
        "    lemmatize_words = [lemmatizer.lemmatize(word) for word in filtered_words]\n",
        "  \n",
        "\n",
        "    \n",
        "    \n",
        "    return text, text_p, text_rn, words, filtered_words, port_stemmed,lancat_stemmed,lemmatize_words\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w7nD-BNXei0C"
      },
      "outputs": [],
      "source": [
        "text, text_p, text_rn, words, filtered_words, port_stemmed, lancat_stemmed,lemmatize_words = preprocess('pride_and_prejudice.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X3Jb7xYUei0C"
      },
      "outputs": [],
      "source": [
        "#actual Text\n",
        "print(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "La_VcV-wei0C"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6pBO4c7cei0C"
      },
      "outputs": [],
      "source": [
        "#Remove Punctutaion\n",
        "print(text_p)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P76KHjiLei0C"
      },
      "outputs": [],
      "source": [
        "#Remove Numbers\n",
        "print(text_rn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tjvEoh2Iei0C"
      },
      "outputs": [],
      "source": [
        "#Remove stodwords\n",
        "print(filtered_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o8DEPVq2ei0C"
      },
      "outputs": [],
      "source": [
        "#Porter stemming\n",
        "print(port_stemmed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BQZB3Ej7ei0C"
      },
      "outputs": [],
      "source": [
        "#lancater stemming\n",
        "print(lancat_stemmed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ColHLR0tei0D"
      },
      "outputs": [],
      "source": [
        "#lemmatizers\n",
        "print(lemmatize_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FaCWEN2hei0D"
      },
      "source": [
        "### Accessing Text from the Web"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dwy2If3_ei0D"
      },
      "outputs": [],
      "source": [
        "from urllib.request import urlopen\n",
        "url = \"http://textfiles.com/internet/31.txt\"\n",
        "raw = urlopen(url).read()\n",
        "#type(raw) \n",
        "raw1=bytes.decode(raw)\n",
        "#len(raw)\n",
        "print(raw1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GtqcLdMKei0D"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "NLP Day 2 TS.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}